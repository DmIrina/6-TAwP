{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  label\n0  Here are Thursday's biggest analyst calls: App...      0\n1  Buy Las Vegas Sands as travel to Singapore bui...      0\n2  Piper Sandler downgrades DocuSign to sell, cit...      0\n3  Analysts react to Tesla's latest earnings, bre...      0\n4  Netflix and its peers are set for a ‘return to...      0\n5  Barclays believes earnings for these underperf...      0\n6  Bernstein upgrades Alibaba, says shares can ra...      0\n7  Analysts react to Netflix's strong quarter, wi...      0\n8  Buy Chevron as shares look attractive at these...      0\n9  Morgan Stanley says these global stocks are se...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Here are Thursday's biggest analyst calls: App...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Analysts react to Tesla's latest earnings, bre...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Netflix and its peers are set for a ‘return to...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Barclays believes earnings for these underperf...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Bernstein upgrades Alibaba, says shares can ra...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Analysts react to Netflix's strong quarter, wi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Buy Chevron as shares look attractive at these...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Morgan Stanley says these global stocks are se...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('news.csv')\n",
    "data_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порожні документи: 0\n"
     ]
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.text.str.strip() == ''].shape[0]\n",
    "print(\"Порожні документи:\", total_nulls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "data": {
      "text/plain": "(16990, 2)"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text  label  \\\n0      Here are Thursday's biggest analyst calls: App...      0   \n1      Buy Las Vegas Sands as travel to Singapore bui...      0   \n2      Piper Sandler downgrades DocuSign to sell, cit...      0   \n3      Analysts react to Tesla's latest earnings, bre...      0   \n4      Netflix and its peers are set for a ‘return to...      0   \n...                                                  ...    ...   \n16985  KfW credit line for Uniper could be raised to ...      3   \n16986  KfW credit line for Uniper could be raised to ...      3   \n16987  Russian  https://t.co/R0iPhyo5p7 sells 1 bln r...      3   \n16988  Global ESG bond issuance posts H1 dip as supra...      3   \n16989  Brazil's Petrobras says it signed a $1.25 bill...      3   \n\n                                              clean text  \n0      thursdays biggest analyst calls apple amazon t...  \n1      buy las vegas sands travel singapore builds we...  \n2      piper sandler downgrades docusign sell citing ...  \n3      analysts react teslas latest earnings break wh...  \n4      netflix peers set return growth analysts say g...  \n...                                                  ...  \n16985  kfw credit line uniper could raised bln eur ha...  \n16986  kfw credit line uniper could raised bln eur ha...  \n16987     russian sells bln roubles oneyear repo auction  \n16988  global esg bond issuance posts h dip supranati...  \n16989  brazils petrobras says signed billion sustaina...  \n\n[16990 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Here are Thursday's biggest analyst calls: App...</td>\n      <td>0</td>\n      <td>thursdays biggest analyst calls apple amazon t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n      <td>0</td>\n      <td>buy las vegas sands travel singapore builds we...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n      <td>0</td>\n      <td>piper sandler downgrades docusign sell citing ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Analysts react to Tesla's latest earnings, bre...</td>\n      <td>0</td>\n      <td>analysts react teslas latest earnings break wh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Netflix and its peers are set for a ‘return to...</td>\n      <td>0</td>\n      <td>netflix peers set return growth analysts say g...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16985</th>\n      <td>KfW credit line for Uniper could be raised to ...</td>\n      <td>3</td>\n      <td>kfw credit line uniper could raised bln eur ha...</td>\n    </tr>\n    <tr>\n      <th>16986</th>\n      <td>KfW credit line for Uniper could be raised to ...</td>\n      <td>3</td>\n      <td>kfw credit line uniper could raised bln eur ha...</td>\n    </tr>\n    <tr>\n      <th>16987</th>\n      <td>Russian  https://t.co/R0iPhyo5p7 sells 1 bln r...</td>\n      <td>3</td>\n      <td>russian sells bln roubles oneyear repo auction</td>\n    </tr>\n    <tr>\n      <th>16988</th>\n      <td>Global ESG bond issuance posts H1 dip as supra...</td>\n      <td>3</td>\n      <td>global esg bond issuance posts h dip supranati...</td>\n    </tr>\n    <tr>\n      <th>16989</th>\n      <td>Brazil's Petrobras says it signed a $1.25 bill...</td>\n      <td>3</td>\n      <td>brazils petrobras says signed billion sustaina...</td>\n    </tr>\n  </tbody>\n</table>\n<p>16990 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def preproc_doc(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "\n",
    "preproc_corpus = np.vectorize(preproc_doc)\n",
    "p_corpus = preproc_corpus(data_df['text'])\n",
    "data_df['clean text'] = p_corpus\n",
    "data_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порожні документи: 0\n"
     ]
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.text.str.strip() == ''].shape[0]\n",
    "print(\"Порожні документи:\", total_nulls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "((11893,), (5097,))"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_label, test_label = train_test_split(np.array(data_df['clean text']),\n",
    "                                                                  np.array(data_df['label']), test_size=0.3,\n",
    "                                                                  random_state=0)\n",
    "train_text.shape, test_text.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "data": {
      "text/plain": "    label  train  test\n9       0    174    81\n8       1    584   253\n3       2   2533  1012\n13      3    221   100\n5       4    256   103\n10      5    705   282\n11      6    380   144\n4       7    410   214\n17      8    114    52\n0       9   1082   475\n18     10     46    23\n19     11     29    15\n2      12    331   156\n15     13    319   152\n12     14   1262   560\n6      15    358   143\n1      16    668   317\n16     17    339   156\n7      18   1501   617\n14     19    581   242",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>train</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>174</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>584</td>\n      <td>253</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2533</td>\n      <td>1012</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>221</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>256</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5</td>\n      <td>705</td>\n      <td>282</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>6</td>\n      <td>380</td>\n      <td>144</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>410</td>\n      <td>214</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>8</td>\n      <td>114</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>1082</td>\n      <td>475</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>10</td>\n      <td>46</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>11</td>\n      <td>29</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12</td>\n      <td>331</td>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>13</td>\n      <td>319</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>14</td>\n      <td>1262</td>\n      <td>560</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>15</td>\n      <td>358</td>\n      <td>143</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16</td>\n      <td>668</td>\n      <td>317</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>339</td>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>18</td>\n      <td>1501</td>\n      <td>617</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>581</td>\n      <td>242</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trd = dict(Counter(train_label))\n",
    "tsd = dict(Counter(test_label))\n",
    "\n",
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], columns=['label', 'train', 'test']).sort_values(by=['label'],\n",
    "                                                                                                          ascending=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_train = [wpt.tokenize(document) for document in train_text]\n",
    "tokenized_test = [wpt.tokenize(document) for document in test_text]\n",
    "\n",
    "w2v_num_features = 150\n",
    "w2v_model = word2vec.Word2Vec(tokenized_train, vector_size=w2v_num_features, window=30, min_count=7, sample=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            n_words = n_words + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "def document_vectorize(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in\n",
    "                corpus]\n",
    "    return np.array(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (11893, 150) Test features shape: (5097, 150)\n"
     ]
    }
   ],
   "source": [
    "avg_wv_train_features = document_vectorize(corpus=tokenized_train, model=w2v_model, num_features=w2v_num_features)\n",
    "avg_wv_test_features = document_vectorize(corpus=tokenized_test, model=w2v_model, num_features=w2v_num_features)\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape,\n",
    "      'Test features shape:', avg_wv_test_features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Градієнтний бустинг"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=5, random_state=0)\n",
    "gbc.fit(avg_wv_train_features, train_label)\n",
    "gbc_bow_scores = cross_val_score(gbc, avg_wv_train_features, train_label, cv=5)\n",
    "gbc_bow_mean_score = np.mean(gbc_bow_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (5-fold): [0.48844052 0.48507776 0.47078604 0.48149706 0.46888141]\n",
      "Mean Accuracy: 0.47893655977043315\n",
      "Test Accuracy: 0.45830880910339417\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy (5-fold):', gbc_bow_scores)\n",
    "print('Mean Accuracy:', gbc_bow_mean_score)\n",
    "svm_bow_test_score = gbc.score(avg_wv_test_features, test_label)\n",
    "print('Test Accuracy:', svm_bow_test_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Опорні вектори"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf.fit(avg_wv_train_features, train_label)\n",
    "\n",
    "svm_bow_scores = cross_val_score(clf, avg_wv_train_features, train_label, cv=5)\n",
    "svm_bow_mean_score = np.mean(svm_bow_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (5-fold): [0.52038672 0.5136612  0.49894914 0.50378469 0.49579479]\n",
      "Mean Accuracy: 0.5065153072281255\n",
      "Test Accuracy: 0.5000980969197567\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy (5-fold):', svm_bow_scores)\n",
    "print('Mean Accuracy:', svm_bow_mean_score)\n",
    "svm_bow_test_score = clf.score(avg_wv_test_features, test_label)\n",
    "print('Test Accuracy:', svm_bow_test_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GridSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 1); total time=  15.7s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 1); total time=  15.9s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 1); total time=  15.6s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 1); total time=  15.5s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 1); total time=  15.5s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 2); total time=  54.7s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 2); total time=  54.7s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 2); total time=  54.5s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 2); total time=  54.1s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=10, tfidf__ngram_range=(1, 2); total time=  55.4s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 1); total time=  30.9s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 1); total time=  31.1s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 1); total time=  31.0s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 1); total time=  30.7s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 1); total time=  30.9s\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 2); total time= 1.8min\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 2); total time= 1.8min\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 2); total time= 1.8min\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 2); total time= 1.8min\n",
      "[CV] END clf__learning_rate=0.1, clf__n_estimators=20, tfidf__ngram_range=(1, 2); total time= 1.8min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "gb_pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', GradientBoostingClassifier())])\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__learning_rate': [0.1],\n",
    "    'clf__n_estimators': [10, 20]}\n",
    "\n",
    "gs_gb = GridSearchCV(gb_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_gb = gs_gb.fit(train_text, train_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.6231116342946832\n"
     ]
    }
   ],
   "source": [
    "score = gs_gb.score(test_text, test_label)\n",
    "print('Test Accuracy :', score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}